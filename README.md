# Assignment-S.E 
Summary 1

Software engineering and AI development share many of the same languages and tools, but AI development as an engineering practice is still in early stages. Mining software repositories of AI models enables insight into the current state of AI development. . The extractors have five modules for extracting AI model-specific metadata: model name, associated datasets, references, AI frameworks used, and model domain. We evaluated AIMMX against 7,998 open source models from three sources: model zoos, arXiv AI papers, and state-of-the-art AI papers. AIMMX extracted metadata with 87% precision and 83% recall. 
Despite the obvious benefits of having high-quality metadata, the task of adding it to countless files and records might seem like an incredibly time-consuming and tedious process. You may not have the manpower to comb through hundreds of thousands of text, image, and video files to select relevant keywords. Even if you do have the resources, that might not seem like the best use of your people. Advances in AI and machine learning can minimize human effort and produce excellent results and in that way, AI is finally living up to the hype.

This list provides a basic overview of the types of AI technology that systems use to help with metadata creation:

Natural language processing: These systems can process language in a way that’s very similar to the way people’s brains work. NLP can look for patterns in text, image, and audio files.
Statistical learning: This technology relies upon statistical models to help divine important information from large sets of data.
Neural networks: This kind of tech finds patterns by sifting through information with neural networks that are designed to work a lot like organic, brain neurons.
Deep learning: These advanced systems can sift through layers of information to extract meaning, patterns, and comparisons.

In the past, people associated indexing mostly with text documents. Modern AI isn’t just limited to text files. Case in point… Las Vegas face recognition can identify known cheaters and card-counters in video images. You may have also seen examples of this on popular social networks. Like when Facebook knows that family reunion photo has Aunt Wanda in it and suggests a tag. Language processing can extract meaning from speech in audio files. Combining various techniques will also extra tags from video files.
Thus, you can use AI to help create and add metadata to text, graphics, and video files. For instance, today’s search engines can index and categorize .MP3 and .JPG files as well as .HTML and .PDF files. An intelligent information management system can do the same thing inside of your organization.
Consider some examples from CMSWire of using intelligent systems to categorize various types of files:
Images: The healthcare field has relied heavily on image recognition technology for all sorts of medical scans. Other industries can use this tech to help categorize scanned documents, including handwriting. If you have deposited a hand-written check in the ATM, you have probably seen this kind of image recognition at work.
Audio: Common examples of intelligent speech processing include Amazon Alexa and similar home systems. You have probably also used voice-to-text to compose text messages or request searches on your mobile phone. This same technology can find patterns in your company’s audio recordings.
Video: Analyzing video files combines the AI tech that’s used to process images, text, and audio. For example, you might tag everybody at a meeting by using facial recognition of a recording. Similarly, you may set time indexes of a video to make it easier to find the exact moment when a certain topic got discussed.

AI can help reduce effort and, in some cases, improve the quality of your metadata. Mostly, intelligent systems can make projects possible that you may lack the time or funds to accomplish quickly if you had to do them manually. Even better, these systems learn as they work, so they can provide increasingly better and more useful results over time. Since the machines never get tired or bored, they can also help minimize and eliminate the kinds of mistakes that people are prone to making.
Here’s a simple example: How fast could the fastest data worker look through 500 documents to find instances of social security numbers and then tag those documents as sensitive? Maybe a few days. Intelligent information management AI can do it in minutes, if not seconds. That’s the kind of power we’re talking about here.
You should still involve various stakeholders to determine which kinds of metadata you need, in order to create rules within the system and verify results. You can use these rules to help direct both the intelligent software and your quality control teams. Basically, the higher the risk of specific information, the more you may need to rely upon people to normalize the intelligence with governance rules and quality verification.
You might prioritize various kinds of information, so you can devote more time to the specific documents that carry the most value and associated risks. Also, you might start testing your smart systems with low-priority information, so both you and your AI system can learn to work together better.


Summary 2

SUMMARIZING
                  The largest portion of video game market is targeting mobile devices and streaming services  for increasing the complexity. Unitylinter is a tool to detect bad smells in video games and pretain to performance maintainbility and performance issues. Result of our empericle ivestigation indicates that developers well recieved performance and behevarial and maitain related issues are more controversial.
                                  Video game development industry is largest share of software development industry in 2018 the increasement in videogame development ratio is 10% in one year now a days videogame industries are cahnging thier platform from console and computers to mobile devices even the video game streaming is a top trend currect time. video game market is a kind of niche on stacoverflow over 1.2 million tags are abvout android and only 50ks are related to unity this is now clear that in this context developers need a huge support. 
Static code analysis tools SCAT like linter first develop by jhonson for C language to analyzze the problems in  code, SCAT general propose is to find bugs in many programming languages. Unity linter statically analyzes the code and other arttifacts of a unityvideogame, and can detect 7 types of bad smells such smells covers  different quality  aspects  of videogame development, namely performance, maintainbility, and correct behaviour.
Unity is a framework for developing the 2D and 3D games. This framework is based on graphics, the plot of games like objects and area and movements are graphically designed with the Unity, Unity allows  developers to decorate an object with different kind of verious components.                                                                              


Summary 3

Developers are using third party apps for software developent for adding new patches and features but there are alot of problems faced by developers some times the software cause break down or caught bugs.
Now a days software development are bieng complex and most of the softwres are not built from scratch but rather leverage others codeToday’s software systems are large and complex.

Many of these software systems are not built from scratch, but rather leverage others code that has been built in the past to accelerate their own  development. One particular driver of this code reuse is the growing  popularity of software ecosystems such as Node.js PackageManager (npm)  which provides a platform for developers to share their own and use others’ code.
Code reuse has m any advantages, including allowing software  systems to be developedfaster, include richer features, and even  achieve higher quality. However, this often comes at an increased cost of having to manage these dependencies. Specifically, as the software evolves (and its dependencies do as well), updating these dependencies can become more risky.
To ensure the stability and quality of newly released dependencies, developers often run their own tests. This has proven tobe a good solution and some tools  support the automation of such approaches. However, in many cases,  developers are still forced to“rollback” updates to packages because they introduce regression in their system functionality.

To detect breakage-inducing versions, we execute the tests of dependent projects that depend on the priorversion of the target dependency.For those tests that passon the priorversion,were execute them after updating the target dependency to the newer version.Tests that pass the execution on the prior version but not  the execution on the newer version may indicate that the newer  version has introduced a breakage.
The technique runs tests from dependent projects before and  after updating a target dependency from a prior version to a newer  version. Unless an update is intentionally breaking backwards compatibility (e.g., a major release), the tests from the prior version should continue to pass in the new version.

